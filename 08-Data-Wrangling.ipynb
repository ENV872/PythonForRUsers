{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Data Wrangling II (in Python & Pandas)\n",
    "*Environmental Data Analytics | John Fay*<br>\n",
    "*Spring 2019*\n",
    "\n",
    "## LESSON OBJECTIVES\n",
    "*[This notebook mimics the `08_DataWrangling.Rmd` R markdown document...]*\n",
    "1. Wrangle datasets with Python and Pandas functions\n",
    "2. Compare R's `dplyr` to Python's `pandas` package with respect to wrangling data.\n",
    "3. Apply data wrangling skills to a real-world example dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SET UP YOUR DATA ANALYSIS SESSION\n",
    "* Import Pandas\n",
    "* Read in the following datasets in Pandas dataframes\n",
    " * `NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv`\n",
    " * `NTL-LTER_Lake_Nutrients_Raw.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import pandas and numpy\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.phys.data.PeterPaul <- read.csv(\"./Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv\")\n",
    "NTL_physical = pd.read_csv('../Data/Processed/NTL-LTER_Lake_ChemistryPhysics_PeterPaul_Processed.csv')\n",
    "\n",
    "#NTL.nutrient.data <- read.csv(\"./Data/Raw/NTL-LTER_Lake_Nutrients_Raw.csv\")\n",
    "NTL_nutrient= pd.read_csv('../Data/Raw/NTL-LTER_Lake_Nutrients_Raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REVIEW OF BASIC DATA EXPLORATION AND WRANGLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# head(NTL.phys.data.PeterPaul)\n",
    "NTL_physical.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colnames(NTL.phys.data.PeterPaul)\n",
    "NTL_physical.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim(NTL.phys.data.PeterPaul)\n",
    "NTL_physical.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(NTL.phys.data.PeterPaul$comments)\n",
    "NTL_physical['comments'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class(NTL.phys.data.PeterPaul$sampledate)\n",
    "NTL_physical['sampledate'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Format `sampledate` as date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.phys.data.PeterPaul$sampledate <- as.Date(NTL.phys.data.PeterPaul$sampledate, format = \"%m/%d/%y\")\n",
    "NTL_physical['sampledate'] = pd.to_datetime(NTL_physical['sampledate'],format = \"%m/%d/%y\")\n",
    "NTL_physical['sampledate'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Select Peter and Paul Lakes from the nutrient dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul <- filter(NTL.nutrient.data, lakename == \"Paul Lake\" | lakename == \"Peter Lake\")\n",
    "NTL_nutrient_PP = NTL_nutrient.query('lakename == \"Paul Lake\" | lakename == \"Peter Lake\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data summaries for nutrient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#head(NTL.nutrient.data.PeterPaul)\n",
    "NTL_nutrient_PP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#colnames(NTL.nutrient.data.PeterPaul)\n",
    "NTL_nutrient_PP.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dim(NTL.nutrient.data.PeterPaul)\n",
    "NTL_nutrient_PP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary(NTL.nutrient.data.PeterPaul$comments)\n",
    "NTL_nutrient_PP['comments'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class(NTL.nutrient.data.PeterPaul$sampledate)\n",
    "NTL_nutrient_PP['sampledate'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul$sampledate <- as.Date(NTL.nutrient.data.PeterPaul$sampledate, format = \"%m/%d/%y\")\n",
    "NTL_nutrient_PP['sampledate'] = pd.to_datetime(NTL_nutrient['sampledate'],format= \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Oops! What's that warning??_**<br>\n",
    "_The warning you get has to do with the fact that we are applying a calculation to a virtual **view** of a dataframe, i.e. a selection of records. `NTL_nutrient_PP` is only showing records corresponding to Paul Lake and Peter Lake, but it still knows it's linked to the full dataset. You can read more about this in the link provided in the error, or [this page](https://medium.com/dunder-data/selecting-subsets-of-data-in-pandas-part-4-c4216f84d388) has a better explanation of the isses._ \n",
    "\n",
    "_While this warning (not error) has no impact on our analysis, if you wanted to be super careful, an easy workaround is to create a \"deep\" copy of our subset dataframe. When we do this, it creates a new object, breaking the link..._ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add \"copy(deep=True)\" to our query to ensure that our product is its own object, not a virtual view of another\n",
    "NTL_nutrient_PP = NTL_nutrient.query('lakename == \"Paul Lake\" | lakename == \"Peter Lake\"').copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now perform the date conversion. No error!\n",
    "NTL_nutrient_PP['sampledate'] = pd.to_datetime(NTL_nutrient_PP['sampledate'],format= \"%m/%d/%y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTL_nutrient_PP['sampledate'].dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Save processed nutrient file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write.csv(NTL.nutrient.data.PeterPaul, row.names = FALSE, file = \"./Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaul_Processed.csv\")\n",
    "NTL_nutrient_PP.to_csv(\"../Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaul_Processed2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove columns that are not of interest for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.phys.data.PeterPaul.skinny <- select(NTL.phys.data.PeterPaul, \n",
    "#                                         lakename, daynum, year4, sampledate:irradianceDeck)\n",
    "NTL_physical_skinny = NTL_physical[['lakename','daynum','year4','sampledate','depth', 'temperature_C',\n",
    "                                    'dissolvedOxygen','irradianceWater','irradianceDeck']]\n",
    "\n",
    "#NTL.nutrient.data.PeterPaul.skinny <- select(NTL.nutrient.data.PeterPaul, \n",
    "#                                             lakename, daynum, year4, sampledate, depth:po4)\n",
    "NTL_nutrient_PP_skinny = NTL_nutrient_PP[['lakename','daynum','year4','sampledate','depth', 'tn_ug',\n",
    "                                          'tp_ug', 'nh34', 'no23', 'po4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.phys.data.PeterPaul.skinny <- select(NTL.phys.data.PeterPaul, \n",
    "#                                         lakename, daynum, year4, sampledate:irradianceDeck)\n",
    "NTL_physical_skinny = NTL_physical.loc[:,['lakename','daynum','year4','sampledate','depth', 'temperature_C',\n",
    "                                          'dissolvedOxygen','irradianceWater','irradianceDeck']]\n",
    "\n",
    "#NTL.nutrient.data.PeterPaul.skinny <- select(NTL.nutrient.data.PeterPaul, \n",
    "#                                             lakename, daynum, year4, sampledate, depth:po4)\n",
    "NTL_nutrient_PP_skinny = NTL_nutrient_PP.loc[:,['lakename','daynum','year4','sampledate','depth', 'tn_ug',\n",
    "                                                'tp_ug', 'nh34', 'no23', 'po4']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternatively...\n",
    "NTL_phys_skinny = NTL_physical.drop(columns=['lakeid','comments'])\n",
    "NTL_nutrient_PP_skinny = NTL_nutrient_PP.drop(columns=['lakeid','depth_id','comments'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TIDY DATASETS\n",
    "\n",
    "For most situations, data analysis works best when you have organized your data into a tidy dataset. A tidy dataset is defined as: \n",
    "\n",
    "- Each variable is a column\n",
    "- Each row is an observation (e.g., sampling event from a specific date and/or location)\n",
    "- Each value is in its own cell\n",
    "\n",
    "However, there may be situations where we want to reshape our dataset, for example if we want to facet numerical data points by measurement type (more on this in the data visualization unit). We can program this reshaping in a few short lines of code using the package tidyr, which is conveniently included in the tidyverse package. \n",
    "\n",
    "→ Help on tidying Pandas dataframes is [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/reshaping.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Gather**, or **`melt`** in Pandas, nutrient data into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a look at what we're going to melt\n",
    "NTL_nutrient_PP_skinny.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul.gathered <- \n",
    "#   gather(NTL.nutrient.data.PeterPaul.skinny, \"nutrient\", \"concentration\", tn_ug:po4)\n",
    "NTL_nutrient_PP_gathered = NTL_nutrient_PP_skinny.melt(id_vars=['lakename','year4','daynum','sampledate','depth'],\n",
    "                                                       var_name='nutrient',\n",
    "                                                       value_name='concentration',\n",
    "                                                       value_vars=['tn_ug','tp_ug','po4','nh34','no23','po4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ _Unlike R's `gather`, Pandas' `melt` does not keep \"placeholder `NA` values\", i.e. the counts are different_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count(NTL.nutrient.data.PeterPaul.gathered)\n",
    "NTL_nutrient_PP_gathered['nutrient'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ _Just to make sure, we'll drop NAs and reexamine the counts_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul.gathered <- subset(NTL.nutrient.data.PeterPaul.gathered, !is.na(concentration))\n",
    "NTL_nutrient_PP_gathered = NTL_nutrient_PP_gathered[~NTL_nutrient_PP_gathered['concentration'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count(NTL.nutrient.data.PeterPaul.gathered, nutrient)\n",
    "NTL_nutrient_PP_gathered['nutrient'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write.csv(NTL.nutrient.data.PeterPaul.gathered, row.names = FALSE, \n",
    "#          file =\"./Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaulGathered_Processed.csv\")\n",
    "NTL_nutrient_PP_gathered.to_csv(\"../Data/Processed/NTL-LTER_Lake_Nutrients_PeterPaulGathered_Processed2.csv\",\n",
    "                                index=False\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Spread (`pivot_table`)** nutrient data into separate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul.spread <- spread(NTL.nutrient.data.PeterPaul.gathered, nutrient, concentration)\n",
    "NTL_nutrient_PP_spread = NTL_nutrient_PP_gathered.pivot_table(columns='nutrient',\n",
    "                                                              index=['lakename','year4','daynum','sampledate','depth'],\n",
    "                                                              values='concentration').reset_index()\n",
    "NTL_nutrient_PP_spread.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Split** components of cells into multiple columns. (Opposite of 'separate' is 'unite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.nutrient.data.PeterPaul.dates <- separate(NTL.nutrient.data.PeterPaul.skinny, sampledate, c(\"Y\", \"m\", \"d\"))\n",
    "#NTL_nutrient_PP['dates'] = \n",
    "dtIndex = pd.DatetimeIndex(NTL_nutrient_PP_skinny['sampledate'])\n",
    "NTL_nutrient_PP_dates = NTL_nutrient_PP_skinny.copy(deep='True')\n",
    "NTL_nutrient_PP_dates['Y']=dtIndex.year\n",
    "NTL_nutrient_PP_dates['m']=dtIndex.month\n",
    "NTL_nutrient_PP_dates['d']=dtIndex.day\n",
    "\n",
    "NTL_nutrient_PP_dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "→ _The above is specific to splitting dates. However, we can use pandas' `str` function which allows us to apply any string function to the values in the column_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the lakename column into two, using the space as separator\n",
    "NTL_nutrient_PP_skinny['lakename'].str.split(' ',expand=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOINING MULTIPLE DATASETS\n",
    "In many cases, we will want to combine datasets into one dataset. If all column names match, the data frames can be combined with the [`concat`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html) function. If some column names match and some column names don't match, we can combine the data frames using a **\"merge\"** function according to common conditions that exist in the matching columns. We will demonstrate this with the NTL-LTER physical and nutrient datasets, where we have specific instances when physical and nutrient data were collected on the same date, at the same lake, and at the same depth. \n",
    "\n",
    "In Pandas, there are several types of join functions: \n",
    "\n",
    "* `left`: use only keys from left frame, similar to a SQL left outer join; preserve key order.\n",
    "* `right`: use only keys from right frame, similar to a SQL right outer join; preserve key order.\n",
    "* `outer`: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically.\n",
    "* `inner`: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys.\n",
    "\n",
    "Let's say we want to generate a new dataset that contains all possible physical and chemical data for Peter and Paul Lakes. In this case, we want to do a full, or \"outer\" join, joining the tables on the fields that are common: `lakename`,`year4`,`daynum`,`sampledate`,and `depth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, examine the sizes of both dataframes\n",
    "NTL_physical_skinny.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTL_nutrient_PP_skinny.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTL.phys.nutrient.data.PeterPaul <- full_join(NTL.phys.data.PeterPaul.skinny, NTL.nutrient.data.PeterPaul.skinny) \n",
    "NTL_phys_nut_PP = pd.merge(left=NTL_physical_skinny,\n",
    "                          right=NTL_nutrient_PP_skinny,\n",
    "                          how='outer',\n",
    "                          on = [\"lakename\",\"year4\",\"daynum\",\"sampledate\",\"depth\"]\n",
    "                         )\n",
    "#How many records in the joined result? \n",
    "NTL_phys_nut_PP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTL_phys_nut_PP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<font color='blue'>► How many records are in the larger of the two unjoined dataframes. Would you expect an outer join to have more, fewer, or the same number of records as this dataframes? Why? (Check your answer in the cells below...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many records in NTL_physical_skinny?\n",
    "NTL_physical_skinny.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How many records in NTL_nutrient_PP_skinny?\n",
    "NTL_nutrient_PP_skinny.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>► Change the join type to `inner`. Now, how to the sizes compare? What about `left`? `right`?</font>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write.csv(NTL.phys.nutrient.data.PeterPaul, row.names = FALSE, \n",
    "#          file =\"./Data/Processed/NTL-LTER_Lake_Chemistry_Nutrients_PeterPaul_Processed.csv\")\n",
    "NTL_phys_nut_PP.to_csv('../Data/Processed/NTL-LTER_Lake_Chemistry_Nutrients_PeterPaul_Processed2.csv',\n",
    "                       index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GROUPING AND SUMMARIZING DATA (SPLIT-APPLY-COMBINE)\n",
    "\n",
    "dplyr functionality, combined with the pipes operator, allows us to split datasets according to groupings (function: `group_by`), then run operations on those groupings and return the output of those operations. There is a lot of flexibility in this approach, but we will illustrate just one example today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R code we'll mimick:\n",
    "```{r}\n",
    "NTL.PeterPaul.summaries <- \n",
    "  NTL.phys.nutrient.data.PeterPaul %>%\n",
    "  filter(depth == 0) %>%\n",
    "  group_by(lakename) %>%\n",
    "  filter(!is.na(temperature_C) & !is.na(tn_ug) & !is.na(tp_ug)) %>%\n",
    "  summarise(meantemp = mean(temperature_C),  #Can run any funcion here\n",
    "            counts = n(),\n",
    "            sdtemp = sd(temperature_C), \n",
    "            meanTN = mean(tn_ug), \n",
    "            sdTN = sd(tn_ug), \n",
    "            meanTP = mean(tp_ug), \n",
    "            sdTP = sd(tp_ug))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NTL_PP_Summary = (\n",
    "    NTL_phys_nut_PP.\n",
    "    dropna(how='any',axis='rows',subset=['temperature_C','tn_ug','tn_ug']).  #Filter rows with no NA in these fields\n",
    "    query('depth == 0').                 #Select records where depth is 0\n",
    "    groupby('lakename').                 #Group by the `lakename` field\n",
    "    agg({'lakename':['count'],           #Compute aggregate count of outputs\n",
    "         'temperature_C':['mean','std'], #Compute aggregate stats of `temperature_C` values by group\n",
    "         'tn_ug':['mean','std'],         #Compute aggregate stats of `tn_ug` values by group\n",
    "         'tp_ug':['mean','std']          #Compute aggregate stats of `tp_ug` values by group\n",
    "        })\n",
    ")       \n",
    "NTL_PP_Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write.csv(NTL.PeterPaul.summaries, row.names = FALSE, \n",
    "#          file =\"./Data/Processed/NTL-LTER_Lake_Summaries_PeterPaul_Processed.csv\")\n",
    "NTL_PP_Summary.to_csv(\"../Data/Processed/NTL-LTER_Lake_Summaries_PeterPaul_Processed2.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
